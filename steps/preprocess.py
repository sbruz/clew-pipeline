import os
import re
import time
import json
import spacy
from typing import Optional
from openai import OpenAI, OpenAIError, APIConnectionError, RateLimitError, AuthenticationError
from utils.sentence_splitter import split_old_into_sentences
from steps.export import fetch_localized_title_and_author
from schemas.translation_schema import (
    ChapterParagraphSentence,
    Sentence,
    ChapterItemWithSentences,
    ChapterStructureWithSentences,
    WordItem,
    ParagraphWordAnalysis,
    SentenceOriginal,
    ChapterParagraphSentenceOriginal,
    ChapterParagraphSentenceTranslated,
    ChapterItemWithTranslatedSentences,
    ChapterStructureTranslatedSentences,
)
from schemas.chapter_schema import ChapterStructure
from schemas.paragraph_split import ParagraphParts


def format_text_with_openai(text, lang="en", max_chars=4000) -> str:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    system_prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –≤ –ø–æ—Ä—è–¥–æ–∫, –Ω–µ –º–µ–Ω—è—è –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è:\n"
        "- –£–¥–∞–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –∞–±–∑–∞—Ü–µ–≤.\n"
        "- –£–¥–∞–ª–∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–æ–π –∞–±–∑–∞—Ü–∞ –∏ –º–µ–∂–¥—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏.\n"
        "- –ó–∞–º–µ–Ω–∏ –¥–≤–æ–π–Ω—ã–µ –∏ —Ç—Ä–æ–π–Ω—ã–µ —Ç–∏—Ä–µ –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –≤–æ–∫—Ä—É–≥.\n"
        "- –ù–µ –∏–∑–º–µ–Ω—è–π —Ç–µ–∫—Å—Ç, –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n"
        "- –†–∞–∑–¥–µ–ª—è–π –∞–±–∑–∞—Ü—ã –¥–≤—É–º—è –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏ —Å—Ç—Ä–æ–∫–∏.\n"
    )

    try:
        print("‚è≥ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI...")
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": system_prompt},
                # <--- –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–º–∏—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
                {"role": "user", "content": text[:max_chars]}
            ],
            temperature=0.2
        )
        print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç OpenAI.")
        return response.choices[0].message.content

    except RateLimitError:
        print("‚õî –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI. –ü–æ–¥–æ–∂–¥–∏ –∏ –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
    except AuthenticationError:
        print("‚õî –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å OPENAI_API_KEY.")
    except APIConnectionError:
        print("‚õî –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º. –ü—Ä–æ–≤–µ—Ä—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏–ª–∏ API-–¥–æ—Å—Ç—É–ø.")
    except OpenAIError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {str(e)}")
    except Exception as e:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

    return ""


def run(text, lang, max_chars):
    print("üìò –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")
    start = time.time()
    formatted = format_text_with_openai(text, lang, max_chars)
    end = time.time()

    if not formatted:
        print("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")
        return ""

    print(f"‚úÖ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {round(end - start, 2)} —Å–µ–∫—É–Ω–¥.\n")
    print("üìÑ –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
    print(formatted[:1000])
    return formatted


def split_paragraphs_with_openai(text, lang="en", max_chars=4000) -> str:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    system_prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.\n"
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã:\n"
        "- –î–µ–ª–∞–π –∞–±–∑–∞—Ü—ã –¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤.\n"
        "- –ï—Å–ª–∏ –∞–±–∑–∞—Ü –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä—è–º—É—é —Ä–µ—á—å, –≤—Å—ë —Ä–∞–≤–Ω–æ –º–æ–∂–µ—à—å —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ."
        "- –ê–±–∑–∞—Ü –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n"
        "- –û—Ç–¥–µ–ª—è–π –∫–∞–∂–¥—ã–π –∞–±–∑–∞—Ü –¥–≤—É–º—è –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫–∏.\n"
        "- –ù–µ –º–µ–Ω—è–π –∏ –Ω–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ç–µ–∫—Å—Ç.\n"
        "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."
    )

    try:
        print("‚è≥ –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä–∞–∑–±–∏–≤–∫—É –≤ OpenAI...")
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text[:max_chars]}
            ],
            temperature=0.3
        )
        print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç OpenAI.")
        return response.choices[0].message.content

    except RateLimitError:
        print("‚õî –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI.")
    except AuthenticationError:
        print("‚õî –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å API-–∫–ª—é—á.")
    except APIConnectionError:
        print("‚õî –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º.")
    except OpenAIError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {str(e)}")
    except Exception as e:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

    return ""


def split_into_paragraphs(book_id, lang, max_chars):
    from utils.supabase_client import get_supabase_client
    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º splitted_text –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        "splitted_text").eq("id", book_id).single().execute()
    text = response.data.get("splitted_text")
    if not text:
        print("‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏.")
        return

    result = split_paragraphs_with_openai(text, lang, max_chars)

    if not result:
        print("‚ö†Ô∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∞–±–∑–∞—Ü—ã –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.")
        return

    print("üì§ –°–æ—Ö—Ä–∞–Ω—è–µ–º separated_text...")
    supabase.table("books").update(
        {"separated_text": result}).eq("id", book_id).execute()
    print("‚úÖ –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∞–±–∑–∞—Ü—ã –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.")


def group_into_chapters(book_id: int, lang: str, max_chars: int):
    from utils.supabase_client import get_supabase_client
    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º separated_text_verified –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        "separated_text_verified").eq("id", book_id).single().execute()
    text: Optional[str] = response.data.get("separated_text_verified")

    if not text:
        print("‚ùå –¢–µ–∫—Å—Ç –¥–ª—è –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    system_prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –≥–ª–∞–≤—ã –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.\n"
        "–†–∞–∑–±–µ–π —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∞–≤–∏–ª–∞–º:\n"
        "- –ù–æ–≤–∞—è –≥–ª–∞–≤–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Ç–∞–º, –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Å—Ü–µ–Ω–∞ –∏–ª–∏ –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏—Ç–∞—Ç–µ–ª—è –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ.\n"
        "- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –≥–ª–∞–≤—ã: –æ—Ç 15 –¥–æ 50 –∞–±–∑–∞—Ü–µ–≤.\n"
        "- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî —Å–º—ã—Å–ª–æ–≤–æ–µ –¥–µ–ª–µ–Ω–∏–µ, –∞ –Ω–µ –¥–ª–∏–Ω–∞.\n"
        "- –ù–µ –∏–∑–º–µ–Ω—è–π —Ç–µ–∫—Å—Ç –∞–±–∑–∞—Ü–µ–≤.\n"
        "- –ù–∞—á–∏–Ω–∞–π –Ω—É–º–µ—Ä–∞—Ü–∏—é –∞–∑–±–∞—Ü–µ–≤ —Å 1 –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã.\n"
        "- –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ JSON, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É.\n"
    )

    try:
        print("‚è≥ –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ GPT –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ –≥–ª–∞–≤—ã...")

        completion = client.beta.chat.completions.parse(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text[:max_chars]}
            ],
            response_format=ChapterStructure,
        )

        chapter_data = completion.choices[0].message.parsed

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É JSON –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Supabase
        json_text = chapter_data.model_dump_json(indent=2)

        supabase.table("books").update(
            {"text_by_chapters": json_text}).eq("id", book_id).execute()
        print("‚úÖ –†–∞–∑–º–µ—Ç–∫–∞ –≥–ª–∞–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–≤–∫–µ –Ω–∞ –≥–ª–∞–≤—ã: {e}")


def simplify_text_for_beginners(book_id: int, lang: str, max_chars: int):
    from utils.supabase_client import get_supabase_client
    from schemas.chapter_schema import ChapterStructure, ChapterItem
    import json

    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º text_by_chapters –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        "text_by_chapters").eq("id", book_id).single().execute()
    original_text = response.data.get("text_by_chapters")

    if not original_text:
        print("‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.")
        return

    original_structure = ChapterStructure.model_validate_json(original_text)
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    lang_map = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–æ–º",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º",
        "de": "–Ω–µ–º–µ—Ü–∫–æ–º",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º"
    }

    # –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî –æ–±–æ–±—â—ë–Ω–Ω–æ
    selected_lang = lang_map.get(lang, "–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º")

    source_lang_map = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–∏–π",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π",
        "de": "–Ω–µ–º–µ—Ü–∫–∏–π",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π"
    }

    # –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî –æ–±–æ–±—â—ë–Ω–Ω–æ
    source_lang_map = lang_map.get(lang, "–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π")

    system_prompt = (
        f"–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ä–∞—Å—Å–∫–∞–∑—á–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≥–ª–∞–≤—ã –∫–Ω–∏–≥ –¥–ª—è –∏–∑—É—á–∞—é—â–∏—Ö {source_lang_map} —è–∑—ã–∫.\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "- –ü–µ—Ä–µ–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ—Ä–æ—Ç–∫–∏–µ, –ª—ë–≥–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É—Ä–æ–≤–µ–Ω—å A2‚ÄìB1).\n"
        "- –°–æ—Ö—Ä–∞–Ω—è–π —Ä–∞–∑–±–∏–≤–∫—É –Ω–∞ –∞–±–∑–∞—Ü—ã ‚Äî –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–π –∏ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –∞–±–∑–∞—Ü—ã.\n"
        f"- –ü–∏—à–∏ –Ω–∞ {selected_lang} —è–∑—ã–∫–µ.\n"
        "- –ü–∏—à–∏ –≤ —Ç—ë–ø–ª–æ–º, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–º —Ç–æ–Ω–µ ‚Äî –∫–∞–∫ –±—É–¥—Ç–æ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—à—å –∏—Å—Ç–æ—Ä–∏—é –¥—Ä—É–≥—É.\n"
        "- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –¥–ª–∏–Ω–Ω—ã—Ö –∏–ª–∏ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω–æ —Å–∞–º–æ –ø–æ —Å–µ–±–µ.\n"
        "- –ù–µ –ø–∏—à–∏ –∫–∞–∫ —É—á–µ–±–Ω–∏–∫. –ü–∏—à–∏ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç.\n"
        "- –ù–µ —É–ø—Ä–æ—â–∞–π –¥–æ —É—Ä–æ–≤–Ω—è —Ä–æ–±–æ—Ç–∞ ‚Äî —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω '—Ç–µ—á—å' –∂–∏–≤–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –ø–æ–Ω—è—Ç–Ω–æ.\n"
        "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –≥–ª–∞–≤—ã –ø–æ —Å—Ö–µ–º–µ ChapterItem. –ë–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
    )

    simplified_chapters = []

    total_chapters = len(original_structure.chapters)

    for idx, chapter in enumerate(original_structure.chapters, start=1):
        progress = round((idx / total_chapters) * 100)
        print(
            f"\nüìò –ì–ª–∞–≤–∞ {chapter.chapter_number} ‚Äî {len(chapter.paragraphs)} –∞–±–∑–∞—Ü–µ–≤ ({progress}%)")

        chapter_attempts = 0
        success = False

        while chapter_attempts < 3 and not success:
            chapter_attempts += 1
            print(f"  üîÑ –ü–æ–ø—ã—Ç–∫–∞ {chapter_attempts}...")

            try:
                chapter_json = chapter.model_dump_json(indent=2)

                completion = client.beta.chat.completions.parse(
                    model="gpt-4.1",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": chapter_json[:max_chars]}
                    ],
                    response_format=ChapterItem,
                )

                simplified = completion.choices[0].message.parsed

                original_count = len(chapter.paragraphs)
                simplified_count = len(simplified.paragraphs)

                if simplified_count != original_count:
                    print(
                        f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞: –æ–∂–∏–¥–∞–ª–æ—Å—å {original_count} –∞–±–∑–∞—Ü–µ–≤, –ø–æ–ª—É—á–µ–Ω–æ {simplified_count}")
                else:
                    print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ. –ê–±–∑–∞—Ü–µ–≤: {simplified_count}")
                    simplified_chapters.append(simplified)
                    success = True

            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ GPT: {e}")

        if not success:
            print(
                f"‚õî –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –≥–ª–∞–≤—ã {chapter.chapter_number}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
            return

    # –°–±–æ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    final_structure = ChapterStructure(chapters=simplified_chapters)
    json_text = final_structure.model_dump_json(indent=2)

    supabase.table("books").update(
        {"text_by_chapters_simplified": json_text}).eq("id", book_id).execute()
    print("\n‚úÖ –í—Å–µ –≥–ª–∞–≤—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")


def translate_text_structure(
    book_id: int,
    source_field: str,
    result_field: str,
    source_lang: str,
    target_lang: str,
    max_chars: int,
    spacy_nlp,
    chapter_number: Optional[int] = None
):
    from utils.supabase_client import get_supabase_client
    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {source_field} –∏ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        f"{source_field}, title, author"
    ).eq("id", book_id).single().execute()

    data = response.data
    text = data.get(source_field)
    title = data.get("title", "")
    author = data.get("author", "")
    year = data.get("year", "")

    if not text:
        print(f"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª–µ {source_field}.")
        return

    original_structure = ChapterStructure.model_validate_json(text)
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    paragraphs_sentences_flat = []
    chapters_result = []

    total_paragraphs = sum(len(ch.paragraphs)
                           for ch in original_structure.chapters)
    translated_count = 0

    previous_paragraphs: list[str] = []

    chapters_to_process = original_structure.chapters
    if chapter_number is not None and chapter_number != -1:
        chapters_to_process = [
            ch for ch in original_structure.chapters if ch.chapter_number == chapter_number
        ]
        if not chapters_to_process:
            print(f"‚ùå –ì–ª–∞–≤–∞ {chapter_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            return

    for chapter in chapters_to_process:
        print(f"\nüìö –ì–ª–∞–≤–∞ {chapter.chapter_number}")
        translated_paragraphs = []

        for paragraph in chapter.paragraphs:
            print(f"  ‚úÇÔ∏è –ê–±–∑–∞—Ü {paragraph.paragraph_number}")

            raw_sentences = split_old_into_sentences(
                paragraph.paragraph_content, source_lang, spacy_nlp)
            para_struct_original = ChapterParagraphSentenceOriginal(
                paragraph_number=paragraph.paragraph_number,
                sentences=[
                    SentenceOriginal(
                        sentence_number=i + 1,
                        sentence_original=s.strip()
                    )
                    for i, s in enumerate(raw_sentences)
                ]
            )
            paragraphs_sentences_flat.append(para_struct_original)

            # –§–æ—Ä–º–∏—Ä—É–µ–º system prompt —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context_prefix = (
                f"–¢—ã –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫. –†–∞–±–æ—Ç–∞–µ—à—å —Å –∫–Ω–∏–≥–æ–π '{title}' –∞–≤—Ç–æ—Ä–∞ {author}.\n"
                f"–Ø–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞: {source_lang}. –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫: {target_lang}.\n"
                "–ü–µ—Ä–µ–≤–µ–¥–∏ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–µ–º –∞–±–∑–∞—Ü–µ.\n"
                "–ü–µ—Ä–µ–≤–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ª–µ–≥–∫–∏–º –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º.\n"
                "–°–æ—Ö—Ä–∞–Ω–∏ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –¥–æ–±–∞–≤–∏–≤ 'sentence_translation' —Ä—è–¥–æ–º —Å 'sentence_original'.\n"
                "–ù–µ —É–¥–∞–ª—è–π –∏ –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n"
            )

            # –î–æ–±–∞–≤–∏–º –¥–æ 2 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∞–±–∑–∞—Ü–µ–≤, –µ—Å–ª–∏ –∏—Ö –æ–±—â–∞—è –¥–ª–∏–Ω–∞ < 300 —Å–∏–º–≤–æ–ª–æ–≤
            # –º–∞–∫—Å–∏–º—É–º 2 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
            context_paragraphs = previous_paragraphs[-2:]
            context_joined = "\n".join(context_paragraphs).strip()

            if context_joined and len(context_joined) <= 300:
                context_prefix += f"\n–ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å):\n{context_joined}\n"
            elif previous_paragraphs:
                context_prefix += f"\n–ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å):\n{previous_paragraphs[-1]}\n"

            system_prompt_translate = context_prefix

            # –ü–µ—Ä–µ–≤–æ–¥
            attempt = 0
            success = False

            while attempt < 3 and not success:
                attempt += 1
                try:
                    print(f"    üåç –ü–µ—Ä–µ–≤–æ–¥ (–ø–æ–ø—ã—Ç–∫–∞ {attempt})...")
                    completion = client.beta.chat.completions.parse(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt_translate},
                            {"role": "user", "content": para_struct_original.model_dump_json(indent=2)[
                                :max_chars]}
                        ],
                        response_format=ChapterParagraphSentenceTranslated
                    )
                    translated_para = completion.choices[0].message.parsed

                    if len(translated_para.sentences) != len(para_struct_original.sentences):
                        print(
                            f"    ‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {len(para_struct_original.sentences)} ‚Üí {len(translated_para.sentences)}")
                    else:
                        translated_paragraphs.append(translated_para)
                        translated_count += 1
                        percent = round(
                            (translated_count / total_paragraphs) * 100)
                        print(
                            f"    ‚úÖ –£—Å–ø–µ—à–Ω–æ. üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {translated_count}/{total_paragraphs} ({percent}%)")
                        success = True

                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ GPT: {e}")
                    time.sleep(2)

            if not success:
                print(
                    f"‚õî –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –∞–±–∑–∞—Ü {paragraph.paragraph_number}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                return

            previous_paragraphs.append(paragraph.paragraph_content.strip())

        chapters_result.append(ChapterItemWithTranslatedSentences(
            chapter_number=chapter.chapter_number,
            paragraphs=translated_paragraphs
        ))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ books_translations –¥–ª—è {target_lang}...")

    full_structure = ChapterStructureTranslatedSentences(
        chapters=chapters_result)
    json_translated = full_structure.model_dump_json(indent=2)

    # üåç –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞
    try:
        localized = fetch_localized_title_and_author(
            title, author, str(year), target_lang)
        localized_title = localized.localized_title
        localized_author = localized.localized_author
        print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ {target_lang}: {localized_title}")
        print(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞ {target_lang}: {localized_author}")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞: {e}")
        localized_title = title
        localized_author = author

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å
    existing = supabase.table("books_translations").select("id").eq(
        "book_id", book_id).eq("language", target_lang).execute()

    if existing.data:
        supabase.table("books_translations").update({
            result_field: json_translated,
            "title": localized_title,
            "author": localized_author
        }).eq("book_id", book_id).eq("language", target_lang).execute()

        print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∑–∞–ø–∏—Å—å.")
    else:
        supabase.table("books_translations").insert({
            "book_id": book_id,
            "language": target_lang,
            result_field: json_translated,
            "title": localized_title,
            "author": localized_author
        }).execute()

        print("üÜï –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å.")

    print("\n‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –≤—Å–µ—Ö –∞–±–∑–∞—Ü–µ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")


def enrich_sentences_with_words(
    book_id: int,
    source_field: str,
    result_field: str,
    source_lang: str,
    target_lang: str,
    max_chars: int,
    paras_number: Optional[int] = None
):
    from utils.supabase_client import get_supabase_client
    supabase = get_supabase_client()

    start_time = time.time()

    print(
        f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {source_field} –∏–∑ books_translations –¥–ª—è –∫–Ω–∏–≥–∏ {book_id} –∏ —è–∑—ã–∫–∞ {target_lang}...")

    response = supabase.table("books_translations").select(
        f"{source_field}"
    ).eq("book_id", book_id).eq("language", target_lang).single().execute()

    text = response.data.get(source_field)
    if not text:
        print(f"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ–ª–µ {source_field}.")
        return

    structure = ChapterStructureWithSentences.model_validate_json(text)
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    total_sentences = sum(len(p.sentences)
                          for c in structure.chapters for p in c.paragraphs)
    enriched_count = 0

    # –°–ª–æ–≤–∞—Ä–∏ —è–∑—ã–∫–æ–≤
    lang_names = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–∏–π",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π",
        "de": "–Ω–µ–º–µ—Ü–∫–∏–π",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π",
        "ru": "—Ä—É—Å—Å–∫–∏–π",
        "zh": "–∫–∏—Ç–∞–π—Å–∫–∏–π",
        "ja": "—è–ø–æ–Ω—Å–∫–∏–π",
        "ko": "–∫–æ—Ä–µ–π—Å–∫–∏–π",
        # –¥–æ–±–∞–≤–ª—è–π –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    }

    readable_source = lang_names.get(source_lang, source_lang)
    readable_target = lang_names.get(target_lang, target_lang)

    # –°–ª–æ–≤–∞—Ä–∏ —è–∑—ã–∫–æ–≤
    lang_names_pr = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–æ–º",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º",
        "de": "–Ω–µ–º–µ—Ü–∫–æ–º",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º",
        "ru": "—Ä—É—Å—Å–∫–æ–º",
        "zh": "–∫–∏—Ç–∞–π—Å–∫–æ–º",
        "ja": "—è–ø–æ–Ω—Å–∫–æ–º",
        "ko": "–∫–æ—Ä–µ–π—Å–∫–æ–º",
        # –¥–æ–±–∞–≤–ª—è–π –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    }

    readable_source_pr = lang_names_pr.get(source_lang, source_lang)
    readable_target_pr = lang_names_pr.get(target_lang, target_lang)

    system_prompt = (
        f"–¢—ã ‚Äî —è–∑—ã–∫–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫.\n"
        f"–Ø–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ ‚Äî {readable_source}. –Ø–∑—ã–∫ –ø–µ—Ä–µ–≤–æ–¥–∞ ‚Äî {readable_target}.\n"
        f"–û—á–∏—Å—Ç–∏ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞–∑–±–µ–π –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–µ —Å–º—ã—Å–ª–æ–≤—ã–µ –∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–º–µ—Å—Ç–µ —Ñ—Ä–∞–∑–æ–≤—ã–µ –≥–ª–∞–≥–æ–ª—ã, –∏–¥–∏–æ–º—ã, –Ω–µ–¥–µ–ª–∏–º—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.\n"
        "–ù–µ –æ–±—ä–µ–¥–∏–Ω—è–π —Å–ª–æ–≤–∞ –≤ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É, –µ—Å–ª–∏ –∏—Ö –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞ –∏ –Ω–µ–≤–µ—Ä–Ω–æ–≥–æ —Ç–æ–ª–∫–æ–≤–∞–Ω–∏—è –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏.\n"
        f"–î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —É–∫–∞–∂–∏:\n"
        f"- o: –æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ {readable_source_pr}\n"
        f"- o_t: –¥–æ—Å–ª–æ–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ {readable_target} (–¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ - –∞—Ä—Ç–∏–∫–ª–µ–π, —á–∞—Å—Ç–∏—Ü, –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ - –∫–æ—Ä–æ—Ç–∫–æ –∏—Ö —Ä–æ–ª—å –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏)\n"
        f"- l: –ª–µ–º–º–∞ –Ω–∞ {readable_source_pr} (–æ—Å—Ç–∞–≤—å –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç 'o' –≤ —Ñ–æ—Ä–º–µ –ª–µ–º–º—ã)\n"
        f"- l_t: –ø–µ—Ä–µ–≤–æ–¥ –ª–µ–º–º—ã –Ω–∞ {readable_target} ‚Äì¬†–µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–π –º–Ω–æ–≥–æ, —É–∫–∞–∂–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –±–µ–∑ —É—á–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ—Å—Ç–∞–≤—å –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ –Ω–µ—Ç –ª–µ–º–º—ã) \n\n"
        f"–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (response_format)."
    )

    for chapter in structure.chapters:
        print(f"\nüìö –ì–ª–∞–≤–∞ {chapter.chapter_number}")
        translated_paragraphs = chapter.paragraphs

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö N –∞–±–∑–∞—Ü–µ–≤ –≥–ª–∞–≤—ã 1
        if (paras_number is not None) and (paras_number != -1):
            if chapter.chapter_number != 1:
                continue
            translated_paragraphs = chapter.paragraphs[:paras_number]

        for paragraph in translated_paragraphs:
            print(
                f"  ‚úÇÔ∏è –ê–±–∑–∞—Ü {paragraph.paragraph_number} ‚Äî {len(paragraph.sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

            attempt = 0
            success = False

            while attempt < 3 and not success:
                attempt += 1
                try:
                    input_data = [
                        {
                            "sentence_number": s.sentence_number,
                            "sentence_original": s.sentence_original  # ,
                            # "sentence_translation": s.sentence_translation
                        }
                        for s in paragraph.sentences
                    ]

                    completion = client.beta.chat.completions.parse(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": json.dumps(
                                input_data, ensure_ascii=False, indent=2)[:max_chars]}
                        ],
                        response_format=ParagraphWordAnalysis,
                    )

                    parsed_paragraph = completion.choices[0].message.parsed
                    parsed_sentences = parsed_paragraph.sentences

                    if len(parsed_sentences) != len(paragraph.sentences):
                        print(
                            f"    ‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —á–∏—Å–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: –æ–∂–∏–¥–∞–ª–æ—Å—å {len(paragraph.sentences)}, –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_sentences)}")
                    else:
                        for sentence in paragraph.sentences:
                            match = next(
                                (s for s in parsed_sentences if s.sentence_number == sentence.sentence_number), None)
                            if not match:
                                raise ValueError(
                                    f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ {sentence.sentence_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                            sentence.words = match.words

                        enriched_count += len(paragraph.sentences)
                        percent = round(
                            (enriched_count / total_sentences) * 100)
                        print(
                            f"    ‚úÖ –£—Å–ø–µ—à–Ω–æ ‚Äî {len(paragraph.sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {enriched_count}/{total_sentences} ({percent}%)")
                        success = True

                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–ª–æ–≤: {e}")

            if not success:
                print(
                    f"‚õî –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞–±–∑–∞—Ü {paragraph.paragraph_number}. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                return

        # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞, –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–∞ 1 –∏ –ø–µ—Ä–≤—ã–µ N –∞–±–∑–∞—Ü–µ–≤
        if (paras_number is not None) and (paras_number != -1):
            break

    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {result_field} –≤ books_translations...")
    json_result = structure.model_dump_json(indent=2)
    supabase.table("books_translations").update(
        {result_field: json_result}
    ).eq("book_id", book_id).eq("language", target_lang).execute()

    print("‚úÖ –†–∞–∑–±–æ—Ä —Å–ª–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")

    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    print(f"‚è± –í—Ä–µ–º—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–ª–æ–≤ –∫–Ω–∏–≥–∏: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")


# –Ω–æ–≤—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —à–∞–≥ –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è - —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è


def split_into_sentences(book_id, lang="en", max_chars=4000):
    from utils.supabase_client import get_supabase_client

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º formated_text –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        "formated_text").eq("id", book_id).single().execute()
    text = response.data.get("formated_text")
    if not text:
        print("‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏.")
        return

    system_prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.\n"
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ä–∞–∑–±–∏—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –ª–æ–≥–∏—á–µ—Å–∫—É—é –∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å.\n"
        "- –û—Å—Ç–∞–≤—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏ —Ä–∞–∑–±–∏–≤–∫–µ –º–æ–≥—É—Ç –ø–æ—Ç–µ—Ä—è—Ç—å —Å–º—ã—Å–ª.\n"
        "- –ù–µ –∏–∑–º–µ–Ω—è–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –∏ –Ω–µ –∏–∑–º–µ–Ω—è–π —Ç–µ–∫—Å—Ç.\n"
        "- –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä—è–º—É—é —Ä–µ—á—å, –≤—Å—ë —Ä–∞–≤–Ω–æ –º–æ–∂–µ—à—å —Ä–∞–∑–±–∏—Ç—å –µ—ë –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –µ—Å–ª–∏ —ç—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é –∏ –ª–æ–≥–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å. –ù–µ –±–æ–π—Å—è —Å—Ç–∞–≤–∏—Ç—å —Ç–æ—á–∫–∏ –¥–∞–∂–µ –≤–Ω—É—Ç—Ä–∏ –∫–∞–≤—ã—á–µ–∫. –í—Å—Ç–∞–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –∫–∞–≤—ã—á–∫–∞–º–∏ –∏ –∑–∞–ø—è—Ç—ã–º–∏ ‚Äî –Ω–µ –ø–æ–≤–æ–¥ –∏–∑–±–µ–≥–∞—Ç—å —Ä–∞–∑–±–∏–µ–Ω–∏—è. –ü—Ä–æ—Å—Ç–æ —Å–æ–±–ª—é–¥–∞–π —Å–º—ã—Å–ª."
        "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."
    )

    try:
        print("‚è≥ –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä–∞–∑–±–∏–≤–∫—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ OpenAI...")
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text[:max_chars]}
            ],
            temperature=0.3
        )
        print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç OpenAI.")
        result = response.choices[0].message.content

    except RateLimitError:
        print("‚õî –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI.")
        return
    except AuthenticationError:
        print("‚õî –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å API-–∫–ª—é—á.")
        return
    except APIConnectionError:
        print("‚õî –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º.")
        return
    except OpenAIError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {str(e)}")
        return
    except Exception as e:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        return

    if not result:
        print("‚ö†Ô∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.")
        return

    print("üì§ –°–æ—Ö—Ä–∞–Ω—è–µ–º splitted_text...")
    supabase.table("books").update(
        {"splitted_text": result}).eq("id", book_id).execute()
    print("‚úÖ –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.")

# –†–£–ß–ù–ê–Ø –†–ê–ó–ë–ò–í–ö–ê –¢–ï–ö–°–¢–ê –ù–ê –ü–ê–†–ê–ì–†–ê–§–´


def split_paragraph_manually(paragraph: str, spacy_nlp, max_length: int = 200, min_chunk_len: int = 30) -> list[str]:
    doc = spacy_nlp(paragraph)
    sentences = [sent.text.strip() for sent in doc.sents]

    new_paragraphs = []
    current = ""

    for i, sentence in enumerate(sentences):
        # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å –≤ –∞–±–∑–∞—Ü–µ, –µ—Å–ª–∏ –±—ã –º—ã –ù–ï –¥–æ–±–∞–≤–∏–ª–∏ —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        remaining_sentences = sentences[i:]
        remaining_length = sum(len(s) for s in remaining_sentences)

        # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
        if len(current) < min_chunk_len:
            current += (" " if current else "") + sentence
            continue

        # –ù–µ –æ—Ç–¥–µ–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ö–≤–æ—Å—Ç
        if remaining_length < min_chunk_len:
            current += (" " if current else "") + sentence
            continue

        # –ï—Å–ª–∏ –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –Ω–µ –ø—Ä–µ–≤—ã—à–∞—è –ª–∏–º–∏—Ç ‚Äî –¥–æ–±–∞–≤–∏–º
        if len(current) + len(sentence) + 1 <= max_length:
            current += (" " if current else "") + sentence
        else:
            # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü
            if current:
                new_paragraphs.append(current.strip())
            current = sentence

    if current:
        new_paragraphs.append(current.strip())

    return new_paragraphs


def verify_separated_text(book_id: int, lang_code: str, nlp):
    from utils.supabase_client import get_supabase_client
    supabase = get_supabase_client()

    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º separated_text –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
    response = supabase.table("books").select(
        "separated_text").eq("id", book_id).single().execute()
    text: Optional[str] = response.data.get("separated_text")

    if not text:
        print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    end_punctuation = re.compile(r"[.!?‚Ä¶]")
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    long_paragraphs = [p for p in paragraphs if len(p) > 200]

    print(
        f"üîç –í—Å–µ–≥–æ –∞–±–∑–∞—Ü–µ–≤: {len(paragraphs)} | –î–ª–∏–Ω–Ω—ã—Ö: {len(long_paragraphs)}")

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    verified_paragraphs = []
    progress_log = []

    for idx, paragraph in enumerate(paragraphs, start=1):
        if len(paragraph) <= 200:
            verified_paragraphs.append(paragraph)
            continue

        status = "failed"
        if end_punctuation.search(paragraph):
            try:
                chunk_count = max(1, round(len(paragraph) / 150))
                system_prompt = (
                    f"–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¥–µ–ª–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞. –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ ‚Äî {lang_code}. "
                    f"–†–∞–∑–¥–µ–ª–∏ –∞–±–∑–∞—Ü –Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏ –ø–æ –¥–ª–∏–Ω–µ, —Ç–∞–∫, —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è —á–∞—Å—Ç—å –∑–∞–∫–∞–Ω—á–∏–≤–∞–ª–∞—Å—å –ø–æ–ª–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º. "
                    f"–¶–µ–ª—å ‚Äî —Ä–∞–∑–±–∏—Ç—å –∞–±–∑–∞—Ü –Ω–∞ {chunk_count} —á–∞—Å—Ç–µ–π. –ù–µ –∏–∑–º–µ–Ω—è–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ. "
                    "–û—Ç–≤–µ—Ç –≤–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ –≤ JSON: {\"parts\": [\"—á–∞—Å—Ç—å 1\", \"—á–∞—Å—Ç—å 2\", ...]}"
                )

                completion = client.beta.chat.completions.parse(
                    model="gpt-4.1",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": paragraph}
                    ],
                    response_format=ParagraphParts
                )

                parts = completion.choices[0].message.parsed.parts
                all_good = True

                for part in parts:
                    if len(part) > 200 and end_punctuation.search(part):
                        manual_parts = split_paragraph_manually(part, nlp)
                        verified_paragraphs.extend(manual_parts)
                        if any(len(p) > 200 for p in manual_parts):
                            status = "failed"
                        else:
                            status = "manual OK"
                        all_good = False
                    else:
                        verified_paragraphs.append(part)

                if all_good:
                    status = "openai OK"

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ GPT: {e}. –ü—Ä–æ–±—É–µ–º –≤—Ä—É—á–Ω—É—é...")
                manual_parts = split_paragraph_manually(paragraph, nlp)
                verified_paragraphs.extend(manual_parts)
                if any(len(p) > 200 for p in manual_parts):
                    status = "failed"
                else:
                    status = "manual OK"
        else:
            verified_paragraphs.append(paragraph)
            status = "failed"

        if len(paragraph) > 200:
            log_index = len(progress_log) + 1
            print(
                f"üß© –î–ª–∏–Ω–Ω—ã–π –∞–±–∑–∞—Ü {log_index}/{len(long_paragraphs)} ‚Üí {status}")
            progress_log.append((log_index, status))

    final_text = "\n\n".join(verified_paragraphs)
    supabase.table("books").update(
        {"separated_text_verified": final_text}
    ).eq("id", book_id).execute()

    print("üì§ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ separated_text_verified.")
