import os
import json
from pathlib import Path
from utils.supabase_client import get_supabase_client
from schemas.export_schema import LocalizedMeta
from openai import OpenAI
from tqdm import tqdm


def fetch_localized_title_and_author(title: str, author: str, year: str, target_lang: str) -> LocalizedMeta:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    lang_map = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–æ–º",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º",
        "de": "–Ω–µ–º–µ—Ü–∫–æ–º",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º",
        "ru": "—Ä—É—Å—Å–∫–æ–º",
        "pt": "–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–º",
        "tr": "—Ç—É—Ä–µ—Ü–∫–æ–º",
        "ja": "—è–ø–æ–Ω—Å–∫–æ–º"

    }

    # –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî –æ–±–æ–±—â—ë–Ω–Ω–æ
    selected_lang = lang_map.get(target_lang, "–∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º")

    system_prompt = (
        f"–í–µ—Ä–Ω–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ '{title}', {author}, {year}, –ø–æ–¥ –∫–æ—Ç–æ—Ä—ã–º –æ–Ω–∞ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å —Ä–∞–Ω–µ–µ –Ω–∞ {selected_lang} —è–∑—ã–∫–µ.\n"
        f"–ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ–±—ã —ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –±—ã–ª–æ –Ω–µ –æ—Ç –¥—Ä—É–≥–æ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞.\n"
        f"–ï—Å–ª–∏ —É —Ç–µ–±—è –Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äì¬†–¥–∞–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –∫–∞–∫ –±—ã —Å–∫–∞–∑–∞–ª–∏ –Ω–æ—Å–∏—Ç–µ–ª–∏ –Ω–∞ {selected_lang} —è–∑—ã–∫–µ.\n"
        f"–¢–∞–∫–∂–µ –¥–∞–π –∏–º—è –∞–≤—Ç–æ—Ä–∞ –Ω–∞ {selected_lang} —è–∑—ã–∫–µ, –µ—Å–ª–∏ –æ–Ω–æ –∏–º–µ–µ—Ç –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã–π –ø–µ—Ä–µ–≤–æ–¥.\n"
        f"–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ö–µ–º–æ–π."
    )

    completion = client.beta.chat.completions.parse(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"title: {title}\nauthor: {author}\nyear: {year}"}
        ],
        response_format=LocalizedMeta,
    )

    return completion.choices[0].message.parsed


def extract_task(task_data, chapter_number, para_number, field):
    chapter = next((c for c in (task_data or {}).get(
        "chapters", []) if c["chapter_number"] == chapter_number), {})
    paragraph = next((p for p in chapter.get("paragraphs", [])
                     if p["paragraph_number"] == para_number), {})
    return paragraph.get(field)


def export_book_json(book_id_start: int, book_id_end: int, source_lang: str, target_langs: list[str]):
    print(
        f"üöÄ –≠–∫—Å–ø–æ—Ä—Ç –∫–Ω–∏–≥ —Å ID {book_id_start}‚Äì{book_id_end} –¥–ª—è —è–∑—ã–∫–æ–≤: {', '.join(target_langs)}")

    supabase = get_supabase_client()
    export_dir = Path("export/content")
    export_dir.mkdir(parents=True, exist_ok=True)

    books_info = []

    for book_id in tqdm(range(book_id_start, book_id_end + 1), desc="üì¶ –ö–Ω–∏–≥–∏", unit="–∫–Ω"):
        exists_response = supabase.table("books").select(
            "id").eq("id", book_id).maybe_single().execute()
        if not exists_response or not exists_response.data:
            print(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫ ID {book_id} ‚Äî –∫–Ω–∏–≥–∏ –Ω–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü–µ books.")
            continue

        # meta_response = supabase.table("books_full_view").select(
        #    "year, words, genre, set"
        # ).eq("id", book_id).single().execute()

        for target_lang in target_langs:

            meta_response = supabase.table("book_export_view").select(
                "year, words, genre, set"
            ).eq("book_id", book_id).eq("language", target_lang).maybe_single().execute()

            if not meta_response.data:
                print(
                    f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ ID {book_id} ‚Äî –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ book_export_view.")
                continue

            meta = meta_response.data
            year = meta.get("year", "")
            words = meta.get("words", "")
            genre = meta.get("genre", "")
            book_set = meta.get("set", "")

            trans_response = supabase.table("books_translations").select(
                "title, author, "
                "text_by_chapters_sentence_translation_words, "
                "text_by_chapters_simplified_sentence_translation_words, "
                "tasks_true_or_false, "
                "tasks_true_or_false_simplified, "
                "tasks_truefalse_howto, "
                "tasks_truefalse_howto_simplified, "
                "tasks_truefalse_howto_words, "
                "tasks_truefalse_howto_words_simplified"
            ).eq("book_id", book_id).eq("language", target_lang).maybe_single().execute()

            if not trans_response.data:
                print(f"   ‚è≠ –Ø–∑—ã–∫ {target_lang} ‚Äî –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–µ—Ç.")
                continue

            data = trans_response.data
            localized_title = data.get("title", "")
            localized_author = data.get("author", "")

            original_text = json.loads(data["text_by_chapters_sentence_translation_words"]) if data.get(
                "text_by_chapters_sentence_translation_words") else None
            simplified_text = json.loads(data["text_by_chapters_simplified_sentence_translation_words"]) if data.get(
                "text_by_chapters_simplified_sentence_translation_words") else None

            # Tasks –ø–æ —Ç–∏–ø–∞–º
            tasks_true_or_false = json.loads(data["tasks_true_or_false"]) if data.get(
                "tasks_true_or_false") else None
            tasks_true_or_false_s = json.loads(data["tasks_true_or_false_simplified"]) if data.get(
                "tasks_true_or_false_simplified") else None

            tasks_how_to_translate = json.loads(data["tasks_truefalse_howto"]) if data.get(
                "tasks_truefalse_howto") else None
            tasks_how_to_translate_s = json.loads(data["tasks_truefalse_howto_simplified"]) if data.get(
                "tasks_truefalse_howto_simplified") else None

            tasks_two_words = json.loads(data["tasks_truefalse_howto_words"]) if data.get(
                "tasks_truefalse_howto_words") else None
            tasks_two_words_s = json.loads(data["tasks_truefalse_howto_words_simplified"]) if data.get(
                "tasks_truefalse_howto_words_simplified") else None

            if not original_text:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ‚Äî —Å–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫—É.")
                original_text = {
                    "chapters": [{
                        "chapter_number": 1,
                        "paragraphs": [{
                            "paragraph_number": 1,
                            "sentences": []
                        }]
                    }]
                }

            for orig_ch in original_text["chapters"]:
                chapter_number = orig_ch["chapter_number"]
                simp_ch = next((c for c in (simplified_text or {}).get(
                    "chapters", []) if c["chapter_number"] == chapter_number), {})

                paragraphs = []
                for orig_p in orig_ch["paragraphs"]:
                    para_num = orig_p["paragraph_number"]

                    simp_p = next((p for p in simp_ch.get(
                        "paragraphs", []) if p["paragraph_number"] == para_num), {})

                    paragraphs.append({
                        "paragraph_number": para_num,
                        "sentences_original": orig_p.get("sentences", []),
                        "sentences_simplified": simp_p.get("sentences", []),
                        "tasks_original": {
                            "true_or_false": extract_task(tasks_true_or_false, chapter_number, para_num, "true_or_false"),
                            "how_to_translate": extract_task(tasks_how_to_translate, chapter_number, para_num, "how_to_translate"),
                            "two_words": extract_task(tasks_two_words, chapter_number, para_num, "two_words")
                        },
                        "tasks_simplified": {
                            "true_or_false": extract_task(tasks_true_or_false_s, chapter_number, para_num, "true_or_false"),
                            "how_to_translate": extract_task(tasks_how_to_translate_s, chapter_number, para_num, "how_to_translate"),
                            "two_words": extract_task(tasks_two_words_s, chapter_number, para_num, "two_words")
                        }
                    })

                chapter_data = {
                    "chapter_number": chapter_number,
                    "paragraphs": paragraphs
                }

                chapter_path = export_dir / \
                    f"book_{book_id}_{source_lang}_{target_lang}_chapter{chapter_number}.json"
                with open(chapter_path, "w", encoding="utf-8") as f:
                    json.dump(chapter_data, f, ensure_ascii=False, indent=2)
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≥–ª–∞–≤–∞: {chapter_path.name}")

            books_info.append({
                "id": f"book_{book_id}",
                "title": localized_title,
                "author": localized_author,
                "year": year,
                "words": words,
                "genre": genre,
                "set": book_set,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "chapters": len(original_text["chapters"])
            })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –∫–Ω–∏–≥ –≤—ã—à–µ –ø–∞–ø–∫–∏ content
    books_path = export_dir.parent / "books.json"
    with open(books_path, "w", encoding="utf-8") as f:
        json.dump(books_info, f, ensure_ascii=False, indent=2)
    print(f"\nüìò –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –°–ø–∏—Å–æ–∫ –∫–Ω–∏–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {books_path}")
