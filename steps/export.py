import os
import json
from pathlib import Path
from utils.supabase_client import get_supabase_client
from schemas.export_schema import LocalizedMeta
from openai import OpenAI


def export_book_json(book_id: int, source_lang: str, target_lang: str):
    supabase = get_supabase_client()

    # üì¶ –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –∫–Ω–∏–≥
    if book_id == -1:
        response = supabase.table("books").select("id").execute()
        ids = [item["id"] for item in response.data]
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ –∫–Ω–∏–≥: {len(ids)}")
        for i in ids:
            export_book_json(i, source_lang, target_lang)
        return

    print(f"üì¶ –≠–∫—Å–ø–æ—Ä—Ç –∫–Ω–∏–≥–∏ ID {book_id}...")

    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
    meta_response = supabase.table("books_full_view").select(
        "title, author, year, words, genre, set"
    ).eq("id", book_id).single().execute()

    if not meta_response.data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏.")
        return

    meta = meta_response.data
    title = meta.get("title", "")
    author = meta.get("author", "")
    year = meta.get("year", "")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    def fetch_json(field):
        response = supabase.table("books").select(
            field).eq("id", book_id).single().execute()
        return json.loads(response.data[field]) if response.data.get(field) else None

    original_text = fetch_json("text_by_chapters_sentence_translation_words")
    simplified_text = fetch_json(
        "text_by_chapters_simplified_sentence_translation_words")
    original_tasks = fetch_json("tasks_truefalse_howto_words")
    simplified_tasks = fetch_json("tasks_truefalse_howto_words_simplified")

    # üîß –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ ‚Äî —Å–æ–∑–¥–∞—ë–º –æ–¥–Ω—É –≥–ª–∞–≤—É —Å –æ–¥–Ω–∏–º –ø—É—Å—Ç—ã–º –∞–±–∑–∞—Ü–µ–º
    if not original_text:
        print("‚ö†Ô∏è –¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É...")
        original_text = {
            "chapters": [{
                "chapter_number": 1,
                "paragraphs": [{
                    "paragraph_number": 1,
                    "sentences": []
                }]
            }]
        }

    # üåç –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞
    try:
        localized = fetch_localized_title_and_author(
            title, author, str(year), target_lang)
        localized_title = localized.localized_title
        localized_author = localized.localized_author
        print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ {target_lang}: {localized_title}")
        print(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞ {target_lang}: {localized_author}")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞: {e}")
        localized_title = title
        localized_author = author

    result = {
        "title": localized_title,
        "author": localized_author,
        "year": year,
        "words": meta.get("words"),
        "genre": meta.get("genre"),
        "set": meta.get("set"),
        "source_lang": source_lang,
        "target_lang": target_lang,
        "chapters": []
    }

    # –°–±–æ—Ä–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    for orig_ch in original_text["chapters"]:
        chapter_number = orig_ch["chapter_number"]
        simp_ch = next((c for c in (simplified_text or {}).get(
            "chapters", []) if c["chapter_number"] == chapter_number), {})
        task_ch = next((c for c in (original_tasks or {}).get(
            "chapters", []) if c["chapter_number"] == chapter_number), {})
        simp_task_ch = next((c for c in (simplified_tasks or {}).get(
            "chapters", []) if c["chapter_number"] == chapter_number), {})

        paragraphs = []
        for orig_p in orig_ch["paragraphs"]:
            para_num = orig_p["paragraph_number"]

            simp_p = next((p for p in simp_ch.get("paragraphs", [])
                          if p["paragraph_number"] == para_num), {})
            task_p = next((p for p in task_ch.get("paragraphs", [])
                          if p["paragraph_number"] == para_num), {})
            simp_task_p = next((p for p in simp_task_ch.get(
                "paragraphs", []) if p["paragraph_number"] == para_num), {})

            paragraphs.append({
                "paragraph_number": para_num,
                "sentences_original": orig_p.get("sentences", []),
                "sentences_simplified": simp_p.get("sentences", []),
                "tasks_original": {
                    "true_or_false": task_p.get("true_or_false"),
                    "how_to_translate": task_p.get("how_to_translate"),
                    "two_words": task_p.get("two_words")
                },
                "tasks_simplified": {
                    "true_or_false": simp_task_p.get("true_or_false"),
                    "how_to_translate": simp_task_p.get("how_to_translate"),
                    "two_words": simp_task_p.get("two_words")
                }
            })

        result["chapters"].append({
            "chapter_number": chapter_number,
            "paragraphs": paragraphs
        })

    # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    output_dir = Path("export") / f"book_{book_id}"
    output_dir.mkdir(parents=True, exist_ok=True)

    full_path = output_dir / f"book_{book_id}_merged.json"
    with open(full_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {full_path}")

    # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –≥–ª–∞–≤–∞
    chapter1_path = output_dir / f"book_{book_id}_merged_chapter1.json"
    chapter1_data = {**result, "chapters": [result["chapters"][0]]}
    with open(chapter1_path, "w", encoding="utf-8") as f:
        json.dump(chapter1_data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {chapter1_path}")


def fetch_localized_title_and_author(title: str, author: str, year: str, target_lang: str) -> LocalizedMeta:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    system_prompt = (
        f"–¢—ã –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. "
        f"–û–ø—Ä–µ–¥–µ–ª–∏, –ø–æ–¥ –∫–∞–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–Ω–∏–≥–∞ '{title}' –∞–≤—Ç–æ—Ä–∞ '{author}' {year} –≥–æ–¥–∞ "
        f"—á–∞—â–µ –≤—Å–µ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞—Å—å –Ω–∞ —è–∑—ã–∫–µ {target_lang}. "
        f"–¢–∞–∫–∂–µ –ø–µ—Ä–µ–≤–µ–¥–∏ –∏–º—è –∞–≤—Ç–æ—Ä–∞ –Ω–∞ —è–∑—ã–∫ {target_lang}, –µ—Å–ª–∏ –æ–Ω–æ –∏–º–µ–µ—Ç –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã–π –ø–µ—Ä–µ–≤–æ–¥. "
        f"–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ö–µ–º–æ–π."
    )

    completion = client.beta.chat.completions.parse(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"title: {title}\nauthor: {author}\nyear: {year}"}
        ],
        response_format=LocalizedMeta,
    )

    return completion.choices[0].message.parsed
