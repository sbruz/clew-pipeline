from schemas.paragraph_translate import HowToTranslateTask
import os
import json
import time
import random
from openai import OpenAI
from pathlib import Path
from utils.supabase_client import get_supabase_client
from schemas.paragraph_tf import ParagraphTFItem, ParagraphTFQuestionOnly
from schemas.paragraph_translate import HowToTranslateTask
from schemas.paragraph_two_words import TwoWordsTask


def generate_paragraph_tasks(book_id: int, source_field: str, result_field: str, target_lang: str, source_lang: str):

    start_time = time.time()
    supabase = get_supabase_client()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    log_path = log_dir / f"log_tasks_book_{book_id}_{target_lang}.txt"

    def log(msg):
        print(msg)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    existing = supabase.table("books_translations").select(result_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    raw_result = existing.data.get(result_field)
    if raw_result and str(raw_result).strip() not in {"", "{}", "[]"}:
        log(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –ø–æ–ª–µ {result_field} —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}, —è–∑—ã–∫–∞ {target_lang}")
        return

    log(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {source_field} –∏–∑ books_translations –¥–ª—è –∫–Ω–∏–≥–∏ {book_id} –∏ —è–∑—ã–∫–∞ {target_lang}...")
    response = supabase.table("books_translations").select(source_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    text_data = response.data.get(source_field)

    if not text_data:
        log(
            f"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∫–Ω–∏–≥–∞ {book_id}, –ø–æ–ª–µ {source_field}, —è–∑—ã–∫ {target_lang}).")
        return

    try:
        data = json.loads(text_data)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {e} (–∫–Ω–∏–≥–∞ {book_id}, –ø–æ–ª–µ {source_field})")
        return

    chapters = data["chapters"]
    total_paragraphs = sum(len(ch["paragraphs"]) for ch in chapters)
    processed_paragraphs = 0
    result = {"chapters": []}

    lang_names_pr = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–æ–º",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º",
        "de": "–Ω–µ–º–µ—Ü–∫–æ–º",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º",
        "ru": "—Ä—É—Å—Å–∫–æ–º",
        "zh": "–∫–∏—Ç–∞–π—Å–∫–æ–º",
        "ja": "—è–ø–æ–Ω—Å–∫–æ–º",
        "ko": "–∫–æ—Ä–µ–π—Å–∫–æ–º",
    }
    readable_target_pr = lang_names_pr.get(target_lang, target_lang)

    for chapter in chapters:
        log_line = f"\nüìò –ì–ª–∞–≤–∞ {chapter['chapter_number']} (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang})"
        print(log_line)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(log_line + "\n")

        chapter_output = {
            "chapter_number": chapter["chapter_number"], "paragraphs": []}

        for paragraph in chapter["paragraphs"]:
            paragraph_text = " ".join(
                sentence["sentence_translation"] for sentence in paragraph["sentences"]).strip()
            if not paragraph_text:
                continue

            processed_paragraphs += 1
            percent = round((processed_paragraphs / total_paragraphs) * 100)
            log(f"  ‚úÇÔ∏è –ê–±–∑–∞—Ü {paragraph['paragraph_number']} ‚Äî {percent}%")
            expected_answer = "true" if random.random() < 0.6 else "false"

            if expected_answer == "true":
                system_prompt = (
                    f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —á—Ç–µ–Ω–∏—é –∫–Ω–∏–≥. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–µ—Ä–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∞–±–∑–∞—Ü–∞.\n\n"
                    f"–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ:\n"
                    f"- –±—ã—Ç—å –Ω–∞ {readable_target_pr} —è–∑—ã–∫–µ;\n"
                    f"- –æ—Ç—Ä–∞–∂–∞—Ç—å —Å—É—Ç—å –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ;\n"
                    f"- –±—ã—Ç—å –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é;\n"
                    f"- –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–º –¥–ª—è –≤–¥—É–º—á–∏–≤–æ–≥–æ —á–∏—Ç–∞—Ç–µ–ª—è;\n"
                    f"- –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º, –¥–æ 7 —Å–ª–æ–≤;\n"
                    f"- –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–µ–π.\n\n"
                    f"–í–µ—Ä–Ω–∏ –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ: {{ question: '...' }}"
                )
            else:
                system_prompt = (
                    f"–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —á—Ç–µ–Ω–∏—é –∫–Ω–∏–≥. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∑–∞–≤–µ–¥–æ–º–æ –ª–æ–∂–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∞–±–∑–∞—Ü–∞.\n\n"
                    f"–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ:\n"
                    f"- –±—ã—Ç—å –Ω–∞ {readable_target_pr} —è–∑—ã–∫–µ;\n"
                    f"- –æ—Ç—Ä–∞–∂–∞—Ç—å –≤—ã–º—ã—à–ª–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ;\n"
                    f"- –±—ã—Ç—å –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é;\n"
                    f"- –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–º –¥–ª—è –≤–¥—É–º—á–∏–≤–æ–≥–æ —á–∏—Ç–∞—Ç–µ–ª—è;\n"
                    f"- –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º, –¥–æ 7 —Å–ª–æ–≤;\n"
                    f"- –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–µ–π.\n\n"
                    f"–í–µ—Ä–Ω–∏ –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ: {{ question: '...' }}"
                )

            attempt = 0
            max_attempts = 2
            success = False
            paragraph_output = {
                "paragraph_number": paragraph["paragraph_number"],
                "true_or_false": None
            }

            while attempt < max_attempts and not success:
                attempt += 1
                try:
                    completion = client.beta.chat.completions.parse(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": paragraph_text[:2000]}
                        ],
                        response_format=ParagraphTFQuestionOnly
                    )
                    q_obj = completion.choices[0].message.parsed

                    cleaned_question = q_obj.question.strip()
                    if cleaned_question.endswith("."):
                        cleaned_question = cleaned_question[:-1]

                    log(
                        f"    üèÖ –í–æ–ø—Ä–æ—Å: {cleaned_question} [{expected_answer}]")

                    paragraph_output["true_or_false"] = ParagraphTFItem(
                        question=cleaned_question,
                        answer=expected_answer
                    ).model_dump()
                    success = True
                except Exception as e:
                    log(
                        f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e} (–∫–Ω–∏–≥–∞ {book_id}, –∞–±–∑–∞—Ü {paragraph['paragraph_number']}, —è–∑—ã–∫ {target_lang})")
                    time.sleep(2)

            chapter_output["paragraphs"].append(paragraph_output)

        result["chapters"].append(chapter_output)

    log(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ {result_field} –¥–ª—è –∫–Ω–∏–≥–∏ {book_id} –∏ —è–∑—ã–∫–∞ {target_lang}...")
    supabase.table("books_translations").update({
        result_field: json.dumps(result, ensure_ascii=False, indent=2)
    }).eq("book_id", book_id).eq("language", target_lang).execute()
    log(f"‚úÖ –í–æ–ø—Ä–æ—Å—ã –ø–æ –∞–±–∑–∞—Ü–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {result_field}.")

    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    summary_msg = f"‚è± –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤: {minutes} –º–∏–Ω {seconds} —Å–µ–∫ (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang}, –ø–æ–ª–µ {result_field})"
    print(summary_msg)
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(summary_msg + "\n")


def add_how_to_translate_tasks(book_id: int, words_field: str, base_task_field: str, result_field: str, target_lang: str, source_lang: str):

    start_time = time.time()
    supabase = get_supabase_client()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    log_path = log_dir / f"log_translate_task_book_{book_id}_{target_lang}.txt"

    def log(msg):
        print(msg)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

    existing = supabase.table("books_translations").select(result_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    raw_result = existing.data.get(result_field)
    if raw_result and str(raw_result).strip() not in {"", "{}", "[]"}:
        log(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –ø–æ–ª–µ {result_field} —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}, —è–∑—ã–∫–∞ {target_lang}")
        return

    log(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {words_field} –∏ {base_task_field} –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}, —è–∑—ã–∫ {target_lang}...")
    words_response = supabase.table("books_translations").select(words_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    tasks_response = supabase.table("books_translations").select(base_task_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()

    words_data = json.loads(words_response.data.get(words_field))
    tasks_data = json.loads(tasks_response.data.get(base_task_field))

    result = {"chapters": []}

    total_paragraphs = sum(len(ch["paragraphs"])
                           for ch in tasks_data["chapters"])
    processed_paragraphs = 0

    for ch_words, ch_tasks in zip(words_data["chapters"], tasks_data["chapters"]):
        log(f"\nüìò –ì–ª–∞–≤–∞ {ch_words['chapter_number']} (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang})")
        updated_paragraphs = []

        for par_words, par_tasks in zip(ch_words["paragraphs"], ch_tasks["paragraphs"]):
            processed_paragraphs += 1
            percent = round((processed_paragraphs / total_paragraphs) * 100)

            paragraph_number = par_words["paragraph_number"]
            log(f"  ‚úÇÔ∏è –ê–±–∑–∞—Ü {paragraph_number} ‚Äî {percent}%")

            word_objects = []
            word_lookup = {}

            for sentence in par_words["sentences"]:
                sid = sentence["sentence_number"]
                for wid, word in enumerate(sentence.get("words", [])):
                    word_id = f"{ch_words['chapter_number']}_{par_words['paragraph_number']}_{sid}_{wid + 1}"

                    o = word["o"].strip()
                    o_t = word["o_t"].strip()

                    if len(o) > 22 or len(o_t) > 22:
                        continue

                    word_obj = {
                        "id": word_id,
                        "o": word["o"],
                        "o_t": word["o_t"]
                    }
                    word_objects.append(word_obj)
                    word_lookup[word_id] = word_obj

            if len(word_objects) < 3:
                log("    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤")
                updated_paragraphs.append({
                    "paragraph_number": paragraph_number,
                    "how_to_translate": None
                })
                continue

            input_json = json.dumps(word_objects, ensure_ascii=False, indent=2)

            system_prompt = (
                "–¢—ã ‚Äî –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\n\n"
                "–£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ —Å –ø–æ–ª—è–º–∏:\n"
                "- id ‚Äî –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª–æ–≤–∞\n"
                "- o ‚Äî —Å–ª–æ–≤–æ –Ω–∞ –∏–∑—É—á–∞–µ–º–æ–º —è–∑—ã–∫–µ\n"
                "- o_t ‚Äî –µ–≥–æ –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —è–∑—ã–∫ —É—á–µ–Ω–∏–∫–∞\n\n"
                "–í—ã–±–µ—Ä–∏ –æ–¥–Ω–æ –º–µ–Ω–µ–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–µ, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –¥–ª—è –∞–±–∑–∞—Ü–∞ —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —É—á–µ–Ω–∏–∫–æ–º —Ç–µ–∫—Å—Ç–∞."
                "–í–µ—Ä–Ω–∏:\n"
                "- correct_id ‚Äî id —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞\n"
                "- incorrect1_id –∏ incorrect2_id ‚Äî id –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞, —Å—Ö–æ–∂–∏—Ö –ø–æ —Ç–∏–ø—É, –Ω–æ –æ—Ç–ª–∏—á–∞—é—â–∏—Ö—Å—è –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é\n\n"
                "–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON –ø–æ —Å—Ö–µ–º–µ"
            )

            attempt = 0
            max_attempts = 2
            success = False
            task_result = None

            while attempt < max_attempts and not success:
                attempt += 1
                try:
                    completion = client.beta.chat.completions.parse(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": input_json[:3000]}
                        ],
                        response_format=HowToTranslateTask
                    )
                    result_obj = completion.choices[0].message.parsed

                    try:
                        correct_word = word_lookup[result_obj.correct_id]
                        incorrect1 = word_lookup[result_obj.incorrect1_id]
                        incorrect2 = word_lookup[result_obj.incorrect2_id]
                    except KeyError as e:
                        log(
                            f"    ‚ö†Ô∏è –û–¥–∏–Ω –∏–∑ ID –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ª–æ–≤–∞—Ä–µ: {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–±–∑–∞—Ü.")
                        break

                    task_result = {
                        "c": result_obj.correct_id,
                        "i1": result_obj.incorrect1_id,
                        "i2": result_obj.incorrect2_id
                    }

                    log(f"    ‚úÖ –í–æ–ø—Ä–æ—Å: –ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è '{correct_word.get('o_t', '<???>')}' ‚Üí {correct_word.get('o', '‚Äî')} | {incorrect1.get('o', '‚Äî')} | {incorrect2.get('o', '‚Äî')}")
                    success = True
                except Exception as e:
                    log(f"    ‚ùå –û—à–∏–±–∫–∞ GPT (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e}")
                    time.sleep(2)

            updated_paragraphs.append({
                "paragraph_number": paragraph_number,
                "how_to_translate": task_result
            })
            time.sleep(1)

        result["chapters"].append({
            "chapter_number": ch_tasks["chapter_number"],
            "paragraphs": updated_paragraphs
        })

    log(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ {result_field}...")
    supabase.table("books_translations").update({
        result_field: json.dumps(result, ensure_ascii=False, indent=2)
    }).eq("book_id", book_id).eq("language", target_lang).execute()
    log(f"‚úÖ –í–æ–ø—Ä–æ—Å—ã 'how to translate' —Å ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–æ–ª–µ {result_field}.")

    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    log(f"‚è± –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {minutes} –º–∏–Ω {seconds} —Å–µ–∫ (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang}, –ø–æ–ª–µ {result_field})")


def add_two_words_tasks(book_id: int, words_field: str, base_task_field: str, result_field: str, target_lang: str):

    start_time = time.time()
    supabase = get_supabase_client()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    log_path = log_dir / f"log_two_words_book_{book_id}_{target_lang}.txt"

    def log(msg):
        print(msg)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

    existing = supabase.table("books_translations").select(result_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    raw_result = existing.data.get(result_field)
    if raw_result and str(raw_result).strip() not in {"", "{}", "[]"}:
        log(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –ø–æ–ª–µ {result_field} —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}, —è–∑—ã–∫–∞ {target_lang}")
        return

    log(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {words_field} –∏ {base_task_field} –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}, —è–∑—ã–∫ {target_lang}...")
    words_response = supabase.table("books_translations").select(words_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()
    tasks_response = supabase.table("books_translations").select(base_task_field).eq(
        "book_id", book_id).eq("language", target_lang).single().execute()

    words_data = json.loads(words_response.data.get(words_field))
    tasks_data = json.loads(tasks_response.data.get(base_task_field))

    result = {"chapters": []}

    total_paragraphs = sum(len(ch["paragraphs"])
                           for ch in tasks_data["chapters"])
    processed_paragraphs = 0

    lang_names_pr = {
        "en": "–∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
        "es": "–∏—Å–ø–∞–Ω—Å–∫–æ–º",
        "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º",
        "de": "–Ω–µ–º–µ—Ü–∫–æ–º",
        "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–º",
        "ru": "—Ä—É—Å—Å–∫–æ–º",
        "zh": "–∫–∏—Ç–∞–π—Å–∫–æ–º",
        "ja": "—è–ø–æ–Ω—Å–∫–æ–º",
        "ko": "–∫–æ—Ä–µ–π—Å–∫–æ–º",
    }
    readable_target_pr = lang_names_pr.get(target_lang, target_lang)

    for ch_words, ch_tasks in zip(words_data["chapters"], tasks_data["chapters"]):
        log(f"\nüìò –ì–ª–∞–≤–∞ {ch_words['chapter_number']} (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang})")
        updated_paragraphs = []

        for par_words, par_tasks in zip(ch_words["paragraphs"], ch_tasks["paragraphs"]):
            processed_paragraphs += 1
            percent = round((processed_paragraphs / total_paragraphs) * 100)

            paragraph_number = par_words["paragraph_number"]
            log(f"  ‚úÇÔ∏è –ê–±–∑–∞—Ü {paragraph_number} ‚Äî {percent}%")

            candidates = []
            word_lookup = {}

            for sentence in par_words["sentences"]:
                sid = sentence["sentence_number"]
                for wid, word in enumerate(sentence.get("words", [])):
                    word_id = f"{ch_words['chapter_number']}_{paragraph_number}_{sid}_{wid + 1}"
                    o_t = word.get("o_t", "").strip()
                    if len(o_t.split()) <= 2 and len(o_t) < 15:
                        candidates.append({"id": word_id, "o_t": o_t})
                        word_lookup[word_id] = o_t

            if len(candidates) < 2:
                log("    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å–ª–æ–≤")
                updated_paragraphs.append({
                    "paragraph_number": paragraph_number,
                    "two_words": None
                })
                continue

            input_json = json.dumps(candidates, ensure_ascii=False, indent=2)

            system_prompt = (
                f"–£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ {readable_target_pr} —è–∑—ã–∫–µ.\n"
                f"–¢–≤–æ—è –∑–∞–¥–∞—á–∞:\n"
                f"1. –ù–∞–π–¥–∏ –¥–≤–∞ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ö–æ–∂–∏ –ø–æ —Ç–∏–ø—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±–∞ ‚Äî –¥–µ–π—Å—Ç–≤–∏—è –∏–ª–∏ –ø—Ä–µ–¥–º–µ—Ç—ã), –Ω–æ —Ä–∞–∑–Ω—ã–µ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é.\n"
                f"2. –í–µ—Ä–Ω–∏ –∏—Ö id –∫–∞–∫ id1 –∏ id2.\n"
                f"3. –ü—Ä–∏–¥—É–º–∞–π —Ç—Ä–µ—Ç—å–µ —Å–ª–æ–≤–æ –Ω–∞ {readable_target_pr} —è–∑—ã–∫–µ, –ø–æ—Ö–æ–∂–µ–µ –ø–æ —Ç–∏–ø—É, –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫ —Ç–µ–º–µ —Ç–µ–∫—Å—Ç–∞.\n\n"
                f"–í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON: {{ id1, id2, invented }}"
            )

            attempt = 0
            max_attempts = 2
            success = False
            task_result = None

            while attempt < max_attempts and not success:
                attempt += 1
                try:
                    completion = client.beta.chat.completions.parse(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": input_json[:3000]}
                        ],
                        response_format=TwoWordsTask
                    )
                    task = completion.choices[0].message.parsed

                    if task.id1 not in word_lookup or task.id2 not in word_lookup:
                        log(
                            f"    ‚ùå –û—à–∏–±–∫–∞: ID {task.id1} –∏–ª–∏ {task.id2} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–±–∑–∞—Ü.")
                        break

                    o1 = word_lookup[task.id1]
                    o2 = word_lookup[task.id2]

                    task_result = {
                        "id1": task.id1,
                        "id2": task.id2,
                        "invented": task.invented
                    }

                    log(f"    ‚úÖ {task.id1}: {o1} | {task.id2}: {o2} ‚Üí {task.invented}")
                    success = True
                except Exception as e:
                    log(f"    ‚ùå –û—à–∏–±–∫–∞ GPT (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e}")
                    time.sleep(2)

            updated_paragraphs.append({
                "paragraph_number": paragraph_number,
                "two_words": task_result
            })
            time.sleep(1)

        result["chapters"].append({
            "chapter_number": ch_tasks["chapter_number"],
            "paragraphs": updated_paragraphs
        })

    log(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ {result_field}...")
    supabase.table("books_translations").update({
        result_field: json.dumps(result, ensure_ascii=False, indent=2)
    }).eq("book_id", book_id).eq("language", target_lang).execute()
    log(f"‚úÖ –í–æ–ø—Ä–æ—Å—ã 'two words + invented' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–æ–ª–µ {result_field}.")

    elapsed = time.time() - start_time
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)
    log(f"‚è± –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {minutes} –º–∏–Ω {seconds} —Å–µ–∫ (–∫–Ω–∏–≥–∞ {book_id}, —è–∑—ã–∫ {target_lang}, –ø–æ–ª–µ {result_field})")
